## 梯度下降

的核心思想就是通过计算代价函数的偏导数，沿着梯度的反方向更新模型参数，从而逐步减小代价函数的值。具体来说，梯度下降的每一步更新可以看作是沿着代价函数曲面向下走一小步，直到找到最小值

$
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
$

* 多维的就是在多个维度上求偏导

## 特征缩放（Feature Scaling）
是数据预处理中的一个重要步骤，主要用于将不同特征的数值范围缩放到相同的尺度，以提高机器学习算法的性能和收敛速度。常见的特征缩放方法包括标准化（Standardization）和归一化（Normalization）。


1. **标准化（Standardization）**：将特征值转换为均值为0，标准差为1的分布。公式如下：
   $
   x' = \frac{x - \mu}{\sigma}
   $


2. **归一化（Normalization）**：将特征值缩放到一个特定的范围（通常是0到1）。公式如下：

   $
   x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
   $